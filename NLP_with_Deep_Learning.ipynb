{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP with Deep Learning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nagendra405/ML/blob/master/NLP_with_Deep_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "vS29nqJfW-rp",
        "colab_type": "code",
        "outputId": "e5f19f73-870c-4290-964e-445681e6b1cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize, sent_tokenize\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Dropout, SpatialDropout1D, Conv1D, GlobalMaxPooling1D, MaxPooling1D, LSTM\n",
        "from keras.layers import Embedding\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.models import load_model\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers import SimpleRNN\n",
        "from keras.layers.wrappers import Bidirectional"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "bqaK-Mxt0qYu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e4c07e11-337c-49ca-e659-0f35a3fb7a94"
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Download a file based on its file ID.\n",
        "#\n",
        "# A file ID looks like: laggVyWshwcyP6kEI-y_W3P8D26sz\n",
        "file_id = '1uo_H7HCEkXsQjjzrkTJ8E_Ns7pm6YKwE'\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('df.csv')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K    1% |▎                               | 10kB 8.5MB/s eta 0:00:01\r\u001b[K    2% |▋                               | 20kB 3.3MB/s eta 0:00:01\r\u001b[K    3% |█                               | 30kB 4.7MB/s eta 0:00:01\r\u001b[K    4% |█▎                              | 40kB 3.7MB/s eta 0:00:01\r\u001b[K    5% |█▋                              | 51kB 4.4MB/s eta 0:00:01\r\u001b[K    6% |██                              | 61kB 5.3MB/s eta 0:00:01\r\u001b[K    7% |██▎                             | 71kB 6.0MB/s eta 0:00:01\r\u001b[K    8% |██▋                             | 81kB 6.7MB/s eta 0:00:01\r\u001b[K    9% |███                             | 92kB 7.5MB/s eta 0:00:01\r\u001b[K    10% |███▎                            | 102kB 6.3MB/s eta 0:00:01\r\u001b[K    11% |███▋                            | 112kB 6.7MB/s eta 0:00:01\r\u001b[K    12% |████                            | 122kB 9.7MB/s eta 0:00:01\r\u001b[K    13% |████▎                           | 133kB 9.7MB/s eta 0:00:01\r\u001b[K    14% |████▋                           | 143kB 16.8MB/s eta 0:00:01\r\u001b[K    15% |█████                           | 153kB 17.2MB/s eta 0:00:01\r\u001b[K    16% |█████▎                          | 163kB 17.1MB/s eta 0:00:01\r\u001b[K    17% |█████▋                          | 174kB 16.7MB/s eta 0:00:01\r\u001b[K    18% |██████                          | 184kB 16.8MB/s eta 0:00:01\r\u001b[K    19% |██████▎                         | 194kB 16.8MB/s eta 0:00:01\r\u001b[K    20% |██████▋                         | 204kB 19.9MB/s eta 0:00:01\r\u001b[K    21% |███████                         | 215kB 20.6MB/s eta 0:00:01\r\u001b[K    22% |███████▎                        | 225kB 20.8MB/s eta 0:00:01\r\u001b[K    23% |███████▋                        | 235kB 20.6MB/s eta 0:00:01\r\u001b[K    24% |████████                        | 245kB 20.8MB/s eta 0:00:01\r\u001b[K    25% |████████▎                       | 256kB 21.0MB/s eta 0:00:01\r\u001b[K    26% |████████▋                       | 266kB 19.1MB/s eta 0:00:01\r\u001b[K    27% |█████████                       | 276kB 20.0MB/s eta 0:00:01\r\u001b[K    29% |█████████▎                      | 286kB 20.0MB/s eta 0:00:01\r\u001b[K    30% |█████████▋                      | 296kB 19.6MB/s eta 0:00:01\r\u001b[K    31% |██████████                      | 307kB 38.8MB/s eta 0:00:01\r\u001b[K    32% |██████████▎                     | 317kB 39.0MB/s eta 0:00:01\r\u001b[K    33% |██████████▋                     | 327kB 39.7MB/s eta 0:00:01\r\u001b[K    34% |███████████                     | 337kB 41.6MB/s eta 0:00:01\r\u001b[K    35% |███████████▎                    | 348kB 36.3MB/s eta 0:00:01\r\u001b[K    36% |███████████▋                    | 358kB 36.3MB/s eta 0:00:01\r\u001b[K    37% |████████████                    | 368kB 43.2MB/s eta 0:00:01\r\u001b[K    38% |████████████▎                   | 378kB 43.0MB/s eta 0:00:01\r\u001b[K    39% |████████████▋                   | 389kB 43.0MB/s eta 0:00:01\r\u001b[K    40% |█████████████                   | 399kB 45.2MB/s eta 0:00:01\r\u001b[K    41% |█████████████▎                  | 409kB 45.5MB/s eta 0:00:01\r\u001b[K    42% |█████████████▋                  | 419kB 45.0MB/s eta 0:00:01\r\u001b[K    43% |██████████████                  | 430kB 34.9MB/s eta 0:00:01\r\u001b[K    44% |██████████████▎                 | 440kB 35.1MB/s eta 0:00:01\r\u001b[K    45% |██████████████▋                 | 450kB 37.4MB/s eta 0:00:01\r\u001b[K    46% |███████████████                 | 460kB 36.3MB/s eta 0:00:01\r\u001b[K    47% |███████████████▎                | 471kB 37.1MB/s eta 0:00:01\r\u001b[K    48% |███████████████▋                | 481kB 37.2MB/s eta 0:00:01\r\u001b[K    49% |████████████████                | 491kB 36.7MB/s eta 0:00:01\r\u001b[K    50% |████████████████▎               | 501kB 37.0MB/s eta 0:00:01\r\u001b[K    51% |████████████████▋               | 512kB 35.2MB/s eta 0:00:01\r\u001b[K    52% |█████████████████               | 522kB 34.8MB/s eta 0:00:01\r\u001b[K    53% |█████████████████▎              | 532kB 44.7MB/s eta 0:00:01\r\u001b[K    54% |█████████████████▋              | 542kB 45.3MB/s eta 0:00:01\r\u001b[K    55% |██████████████████              | 552kB 48.7MB/s eta 0:00:01\r\u001b[K    57% |██████████████████▎             | 563kB 48.3MB/s eta 0:00:01\r\u001b[K    58% |██████████████████▋             | 573kB 47.9MB/s eta 0:00:01\r\u001b[K    59% |███████████████████             | 583kB 48.0MB/s eta 0:00:01\r\u001b[K    60% |███████████████████▎            | 593kB 48.7MB/s eta 0:00:01\r\u001b[K    61% |███████████████████▋            | 604kB 49.6MB/s eta 0:00:01\r\u001b[K    62% |████████████████████            | 614kB 53.5MB/s eta 0:00:01\r\u001b[K    63% |████████████████████▎           | 624kB 54.2MB/s eta 0:00:01\r\u001b[K    64% |████████████████████▋           | 634kB 53.5MB/s eta 0:00:01\r\u001b[K    65% |█████████████████████           | 645kB 52.4MB/s eta 0:00:01\r\u001b[K    66% |█████████████████████▎          | 655kB 50.6MB/s eta 0:00:01\r\u001b[K    67% |█████████████████████▋          | 665kB 42.1MB/s eta 0:00:01\r\u001b[K    68% |██████████████████████          | 675kB 40.1MB/s eta 0:00:01\r\u001b[K    69% |██████████████████████▎         | 686kB 40.0MB/s eta 0:00:01\r\u001b[K    70% |██████████████████████▋         | 696kB 40.2MB/s eta 0:00:01\r\u001b[K    71% |███████████████████████         | 706kB 39.4MB/s eta 0:00:01\r\u001b[K    72% |███████████████████████▎        | 716kB 39.7MB/s eta 0:00:01\r\u001b[K    73% |███████████████████████▋        | 727kB 39.9MB/s eta 0:00:01\r\u001b[K    74% |████████████████████████        | 737kB 39.9MB/s eta 0:00:01\r\u001b[K    75% |████████████████████████▎       | 747kB 39.6MB/s eta 0:00:01\r\u001b[K    76% |████████████████████████▋       | 757kB 40.2MB/s eta 0:00:01\r\u001b[K    77% |████████████████████████▉       | 768kB 49.9MB/s eta 0:00:01\r\u001b[K    78% |█████████████████████████▏      | 778kB 52.5MB/s eta 0:00:01\r\u001b[K    79% |█████████████████████████▌      | 788kB 50.4MB/s eta 0:00:01\r\u001b[K    80% |█████████████████████████▉      | 798kB 50.4MB/s eta 0:00:01\r\u001b[K    81% |██████████████████████████▏     | 808kB 50.9MB/s eta 0:00:01\r\u001b[K    82% |██████████████████████████▌     | 819kB 51.1MB/s eta 0:00:01\r\u001b[K    83% |██████████████████████████▉     | 829kB 52.4MB/s eta 0:00:01\r\u001b[K    85% |███████████████████████████▏    | 839kB 52.2MB/s eta 0:00:01\r\u001b[K    86% |███████████████████████████▌    | 849kB 53.0MB/s eta 0:00:01\r\u001b[K    87% |███████████████████████████▉    | 860kB 49.0MB/s eta 0:00:01\r\u001b[K    88% |████████████████████████████▏   | 870kB 46.3MB/s eta 0:00:01\r\u001b[K    89% |████████████████████████████▌   | 880kB 45.4MB/s eta 0:00:01\r\u001b[K    90% |████████████████████████████▉   | 890kB 46.7MB/s eta 0:00:01\r\u001b[K    91% |█████████████████████████████▏  | 901kB 46.2MB/s eta 0:00:01\r\u001b[K    92% |█████████████████████████████▌  | 911kB 46.4MB/s eta 0:00:01\r\u001b[K    93% |█████████████████████████████▉  | 921kB 46.2MB/s eta 0:00:01\r\u001b[K    94% |██████████████████████████████▏ | 931kB 45.1MB/s eta 0:00:01\r\u001b[K    95% |██████████████████████████████▌ | 942kB 45.4MB/s eta 0:00:01\r\u001b[K    96% |██████████████████████████████▉ | 952kB 43.9MB/s eta 0:00:01\r\u001b[K    97% |███████████████████████████████▏| 962kB 47.0MB/s eta 0:00:01\r\u001b[K    98% |███████████████████████████████▌| 972kB 49.6MB/s eta 0:00:01\r\u001b[K    99% |███████████████████████████████▉| 983kB 50.2MB/s eta 0:00:01\r\u001b[K    100% |████████████████████████████████| 993kB 20.3MB/s \n",
            "\u001b[?25h  Building wheel for PyDrive (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "StqTbjXB08ew",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "reviews_df = pd.read_csv(\"df.csv\")\n",
        "reviews_df[\"review\"] = reviews_df[\"Negative_Review\"] + reviews_df[\"Positive_Review\"]\n",
        "reviews_df[\"is_bad_review\"] = reviews_df[\"Reviewer_Score\"].apply(lambda x: 1 if x < 5 else 0)\n",
        "reviews_df = reviews_df[[\"review\", \"is_bad_review\"]]\n",
        "reviews_df[\"review\"] = reviews_df[\"review\"].apply(lambda x: x.replace(\"No Negative\", \"\").replace(\"No Positive\", \"\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mijZH6531MgK",
        "colab_type": "code",
        "outputId": "beb670fe-c360-48af-d388-66b96841704e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "reviews_df['is_bad_review'].value_counts()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    493457\n",
              "1     22281\n",
              "Name: is_bad_review, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "NT0YKD9S1YCg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "reviews_df_negs = reviews_df[reviews_df['is_bad_review']==1]\n",
        "reviews_df_pos = reviews_df[reviews_df['is_bad_review']==0].sample(n=reviews_df_negs.shape[0],replace=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I7ke1DY11ymy",
        "colab_type": "code",
        "outputId": "46a4efc3-7caf-48b0-e753-6c609d588f0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "print(reviews_df_negs.shape)\n",
        "print(reviews_df_pos.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(22281, 2)\n",
            "(22281, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uZ3mlNeU2SrY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "rev_df = pd.concat((reviews_df_negs,reviews_df_pos))\n",
        "np.random.seed(42)\n",
        "rev_df = rev_df.reindex(np.random.permutation(rev_df.index))\n",
        "rev_df.reset_index(inplace=True, drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f1Wc0QA8BmQB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_data=rev_df.sample(frac=0.1,random_state=42)\n",
        "train_data=rev_df.drop(test_data.index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h-gvVdDtBx58",
        "colab_type": "code",
        "outputId": "ef5473b3-2397-41aa-dad0-58db2282b832",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "print(train_data.shape,test_data.shape)\n",
        "print(train_data.isnull().sum())\n",
        "print(test_data.isnull().sum())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(40106, 2) (4456, 2)\n",
            "review           0\n",
            "is_bad_review    0\n",
            "dtype: int64\n",
            "review           0\n",
            "is_bad_review    0\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "akuYOfTt2AWG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "val_data=train_data.sample(frac=0.1,random_state=42)\n",
        "train_data=train_data.drop(val_data.index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n_q4XUw92332",
        "colab_type": "code",
        "outputId": "2cfaf919-1d75-4dda-de15-601a58eb1004",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "print(train_data.shape,val_data.shape)\n",
        "print(train_data.isnull().sum())\n",
        "print(val_data.isnull().sum())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(36095, 2) (4011, 2)\n",
            "review           0\n",
            "is_bad_review    0\n",
            "dtype: int64\n",
            "review           0\n",
            "is_bad_review    0\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1WxODi8Loo12",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "NUM_WORDS=20000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L86PHVfkEcgi",
        "colab_type": "code",
        "outputId": "0b9a8934-32b3-41b1-af98-f8e73945bc6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words=NUM_WORDS,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'',lower=True)\n",
        "tokenizer.fit_on_texts(train_data['review'])\n",
        "sequences_train = tokenizer.texts_to_sequences(train_data['review'])\n",
        "sequences_valid=tokenizer.texts_to_sequences(val_data['review'])\n",
        "sequences_test=tokenizer.texts_to_sequences(test_data['review'])\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 24992 unique tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Kovpe9nt3j67",
        "colab_type": "code",
        "outputId": "8cbe85c2-5077-40f9-c7e3-b2638a41d18c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "X_train = pad_sequences(sequences_train)\n",
        "X_val = pad_sequences(sequences_valid,maxlen=X_train.shape[1])\n",
        "y_train = np.asarray(train_data['is_bad_review'])\n",
        "y_val = np.asarray(val_data['is_bad_review'])\n",
        "X_test = pad_sequences(sequences_test,maxlen=X_train.shape[1])\n",
        "y_test = np.asarray(test_data['is_bad_review'])\n",
        "print('Shape of X train and X validation tensor:', X_train.shape,X_val.shape)\n",
        "print('Shape of label train and validation tensor:', y_train.shape,y_val.shape)\n",
        "print('Shape of X_test and y_test:', X_test.shape,y_test.shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of X train and X validation tensor: (36095, 689) (4011, 689)\n",
            "Shape of label train and validation tensor: (36095,) (4011,)\n",
            "Shape of X_test and y_test: (4456, 689) (4456,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "poKwA71w422j",
        "colab_type": "code",
        "outputId": "93f29ab0-e4e9-4955-8bd8-344cfab438ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "test_data['is_bad_review'].value_counts()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    2278\n",
              "1    2178\n",
              "Name: is_bad_review, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "nMN35yIP1gLb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def model_evolution(model,X_test,y_test,threshold) :\n",
        "  y_hat = model.predict_proba(X_test)\n",
        "  y_hat = [y_hat > threshold]\n",
        "  y_hat = y_hat[0].reshape(y_test.shape)\n",
        "  print(confusion_matrix(y_test,y_hat))\n",
        "  print(classification_report(y_test,y_hat))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xAjgBJcg3F9H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# training:\n",
        "epochs = 10\n",
        "batch_size = 128\n",
        "n_dim = 256\n",
        "max_input_length = X_train.shape[1] # can be tuned at pad sequence and embedding leyer\n",
        "\n",
        "#Evolution\n",
        "threshold = 0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uRDF24myHjgL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def model_evalute(X, Y) :\n",
        "  # load model\n",
        "  model_ann_one = load_model(X)\n",
        "  # load weights\n",
        "  model_ann_one.load_weights(Y)\n",
        "\n",
        "  # Model Evolution\n",
        "  model_evolution(model_ann_one,X_test,y_test,threshold)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8ZDKl0BXpmNs",
        "colab_type": "code",
        "outputId": "db4a7540-01ed-4e71-c844-859ec75e92a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        }
      },
      "cell_type": "code",
      "source": [
        "# Shallow network- ANN\n",
        "\n",
        "def ANN_one_hidden(NUM_WORDS,n_dim,max_input_length,X_train, y_train,batch_size,epochs,X_val, y_val) :\n",
        "  \n",
        "  model = Sequential()\n",
        "  model.add(Embedding(NUM_WORDS,n_dim, input_length=max_input_length))\n",
        "  model.add(Flatten())\n",
        "  \n",
        "  model.add(Dense(256, activation='relu'))\n",
        "  model.add(Dropout(0.2))\n",
        "  \n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  \n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  \n",
        "  weight_file=\"ann.one.weights.best.hdf5\"\n",
        "  model_name =\"ann.one.h5\"\n",
        "  \n",
        "  callbacks_list = [EarlyStopping(monitor='val_loss', patience=2), ModelCheckpoint(weight_file, monitor='val_acc', verbose=1, save_best_only=True, mode='max')]\n",
        "  \n",
        "  model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(X_val, y_val),callbacks=callbacks_list)\n",
        "  \n",
        "  model.save(model_name)\n",
        "  \n",
        "  return model_name,weight_file\n",
        "\n",
        "# model fitting and saving into drive\n",
        "\n",
        "x, y = ANN_one_hidden(NUM_WORDS,n_dim,max_input_length,X_train, y_train,batch_size,epochs,X_val, y_val)\n",
        "model_evalute(x, y)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 36095 samples, validate on 4011 samples\n",
            "Epoch 1/10\n",
            "36095/36095 [==============================] - 30s 843us/step - loss: 3.0226 - acc: 0.6796 - val_loss: 0.3595 - val_acc: 0.8507\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.85066, saving model to ann.one.weights.best.hdf5\n",
            "Epoch 2/10\n",
            "36095/36095 [==============================] - 27s 738us/step - loss: 0.2872 - acc: 0.8821 - val_loss: 0.3686 - val_acc: 0.8397\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.85066\n",
            "Epoch 3/10\n",
            "36095/36095 [==============================] - 27s 735us/step - loss: 0.1719 - acc: 0.9372 - val_loss: 0.4195 - val_acc: 0.8327\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.85066\n",
            "[[1927  351]\n",
            " [ 300 1878]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.85      0.86      2278\n",
            "           1       0.84      0.86      0.85      2178\n",
            "\n",
            "   micro avg       0.85      0.85      0.85      4456\n",
            "   macro avg       0.85      0.85      0.85      4456\n",
            "weighted avg       0.85      0.85      0.85      4456\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rw9vqeGZ_Gwb",
        "colab_type": "code",
        "outputId": "5d73258b-fe92-41ab-9bcc-174175528a63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "cell_type": "code",
      "source": [
        "def ANN_two_hidden(NUM_WORDS,n_dim,max_input_length,X_train, y_train,batch_size,epochs,X_val, y_val) :\n",
        "  \n",
        "  model = Sequential()\n",
        "  model.add(Embedding(NUM_WORDS,n_dim, input_length=max_input_length))\n",
        "  model.add(Flatten())\n",
        "  \n",
        "  model.add(Dense(256, activation='relu'))\n",
        "  model.add(Dropout(0.2))\n",
        "  \n",
        "  model.add(Dense(256, activation='relu'))\n",
        "  model.add(Dropout(0.2))\n",
        "  \n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  \n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  \n",
        "  weight_file=\"ann.two.weights.best.hdf5\"\n",
        "  model_name =\"ann.two.h5\"\n",
        "  \n",
        "  callbacks_list = [EarlyStopping(monitor='val_loss', patience=2), ModelCheckpoint(weight_file, monitor='val_acc', verbose=1, save_best_only=True, mode='max')]\n",
        "  \n",
        "  model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(X_val, y_val),callbacks=callbacks_list)\n",
        "  \n",
        "  model.save(model_name)\n",
        " \n",
        "  return model_name,weight_file\n",
        "\n",
        "# model fitting and saving into drive\n",
        "\n",
        "x, y = ANN_two_hidden(NUM_WORDS,n_dim,max_input_length,X_train, y_train,batch_size,epochs,X_val, y_val)\n",
        "model_evalute(x, y)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 36095 samples, validate on 4011 samples\n",
            "Epoch 1/10\n",
            "36095/36095 [==============================] - 28s 787us/step - loss: 0.6017 - acc: 0.7591 - val_loss: 0.3449 - val_acc: 0.8519\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.85191, saving model to ann.two.weights.best.hdf5\n",
            "Epoch 2/10\n",
            "36095/36095 [==============================] - 27s 747us/step - loss: 0.2765 - acc: 0.8890 - val_loss: 0.3614 - val_acc: 0.8489\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.85191\n",
            "Epoch 3/10\n",
            "36095/36095 [==============================] - 27s 745us/step - loss: 0.1623 - acc: 0.9398 - val_loss: 0.4660 - val_acc: 0.8355\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.85191\n",
            "[[1890  388]\n",
            " [ 264 1914]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.83      0.85      2278\n",
            "           1       0.83      0.88      0.85      2178\n",
            "\n",
            "   micro avg       0.85      0.85      0.85      4456\n",
            "   macro avg       0.85      0.85      0.85      4456\n",
            "weighted avg       0.85      0.85      0.85      4456\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8dRvCZh-wa8V",
        "colab_type": "code",
        "outputId": "e968a226-f143-4a61-8765-e9efbd7f2ebb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        }
      },
      "cell_type": "code",
      "source": [
        "def CNN_one_layer(NUM_WORDS,n_dim,max_input_length,X_train,y_train,batch_size,epochs,X_val, y_val) :\n",
        "  \n",
        "  model = Sequential()\n",
        "  model.add(Embedding(NUM_WORDS,n_dim, input_length=max_input_length))\n",
        "  model.add(SpatialDropout1D(0.2))\n",
        "  \n",
        "  model.add(Conv1D(256, 3, activation='relu'))\n",
        "  model.add(GlobalMaxPooling1D())\n",
        "  \n",
        "  model.add(Dense(512, activation='relu'))\n",
        "  model.add(Dropout(0.2))\n",
        "  \n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  \n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  weight_file=\"cnn.one.weights.best.hdf5\"\n",
        "  model_name =\"cnn.one.h5\"\n",
        "  \n",
        "  callbacks_list = [EarlyStopping(monitor='val_loss', patience=2), ModelCheckpoint(weight_file, monitor='val_acc', verbose=1, save_best_only=True, mode='max')]\n",
        "  \n",
        "  model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(X_val, y_val),callbacks=callbacks_list)\n",
        "  \n",
        "  model.save(model_name)\n",
        "  \n",
        "  return model_name,weight_file\n",
        "\n",
        "# model fitting and saving into drive\n",
        "\n",
        "x, y = CNN_one_layer(NUM_WORDS,n_dim,max_input_length,X_train, y_train,batch_size,epochs,X_val, y_val)\n",
        "model_evalute(x, y)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "Train on 36095 samples, validate on 4011 samples\n",
            "Epoch 1/10\n",
            "36095/36095 [==============================] - 31s 851us/step - loss: 0.3793 - acc: 0.8278 - val_loss: 0.3260 - val_acc: 0.8651\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.86512, saving model to cnn.one.weights.best.hdf5\n",
            "Epoch 2/10\n",
            "36095/36095 [==============================] - 27s 758us/step - loss: 0.2548 - acc: 0.8993 - val_loss: 0.3407 - val_acc: 0.8629\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.86512\n",
            "Epoch 3/10\n",
            "36095/36095 [==============================] - 27s 760us/step - loss: 0.1700 - acc: 0.9364 - val_loss: 0.3993 - val_acc: 0.8464\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.86512\n",
            "[[1937  341]\n",
            " [ 269 1909]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.85      0.86      2278\n",
            "           1       0.85      0.88      0.86      2178\n",
            "\n",
            "   micro avg       0.86      0.86      0.86      4456\n",
            "   macro avg       0.86      0.86      0.86      4456\n",
            "weighted avg       0.86      0.86      0.86      4456\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ufuWaBhI255J",
        "colab_type": "code",
        "outputId": "739fb9f0-16ff-407d-8d74-f63e0bb35a9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "cell_type": "code",
      "source": [
        "def CNN_lenet(NUM_WORDS,n_dim,max_input_length,X_train, y_train,batch_size,epochs,X_val, y_val) :\n",
        "  \n",
        "  model = Sequential()\n",
        "  model.add(Embedding(NUM_WORDS,n_dim, input_length=max_input_length))\n",
        "  model.add(SpatialDropout1D(0.2))\n",
        "  \n",
        "  model.add(Conv1D(256, 3, activation='relu'))\n",
        "  model.add(Conv1D(256, 3, activation='relu'))\n",
        "  model.add(GlobalMaxPooling1D())\n",
        "  \n",
        "  model.add(Dense(512, activation='relu'))\n",
        "  model.add(Dropout(0.2))\n",
        "  \n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  \n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  weight_file=\"cnn.lenet.weights.best.hdf5\"\n",
        "  model_name =\"cnn.lenet.h5\"\n",
        "  \n",
        "  callbacks_list = [EarlyStopping(monitor='val_loss', patience=2), ModelCheckpoint(weight_file, monitor='val_acc', verbose=1, save_best_only=True, mode='max')]\n",
        "  \n",
        "  model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(X_val, y_val),callbacks=callbacks_list)\n",
        "  \n",
        "  model.save(model_name)\n",
        "  \n",
        "  return model_name,weight_file\n",
        "\n",
        "# model fitting and saving into drive\n",
        "\n",
        "x, y = CNN_lenet(NUM_WORDS,n_dim,max_input_length,X_train, y_train,batch_size,epochs,X_val, y_val)\n",
        "model_evalute(x, y)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 36095 samples, validate on 4011 samples\n",
            "Epoch 1/10\n",
            "36095/36095 [==============================] - 47s 1ms/step - loss: 0.3925 - acc: 0.8188 - val_loss: 0.3360 - val_acc: 0.8537\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.85365, saving model to cnn.lenet.weights.best.hdf5\n",
            "Epoch 2/10\n",
            "36095/36095 [==============================] - 46s 1ms/step - loss: 0.2731 - acc: 0.8902 - val_loss: 0.3416 - val_acc: 0.8556\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.85365 to 0.85565, saving model to cnn.lenet.weights.best.hdf5\n",
            "Epoch 3/10\n",
            "36095/36095 [==============================] - 45s 1ms/step - loss: 0.2030 - acc: 0.9230 - val_loss: 0.3664 - val_acc: 0.8472\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.85565\n",
            "[[1958  320]\n",
            " [ 301 1877]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.86      0.86      2278\n",
            "           1       0.85      0.86      0.86      2178\n",
            "\n",
            "   micro avg       0.86      0.86      0.86      4456\n",
            "   macro avg       0.86      0.86      0.86      4456\n",
            "weighted avg       0.86      0.86      0.86      4456\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kF1980Dj5fa1",
        "colab_type": "code",
        "outputId": "ac83f719-3a25-4b6b-b2b9-55a122de41f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "cell_type": "code",
      "source": [
        "def CNN_alexnet(NUM_WORDS,n_dim,max_input_length,X_train, y_train,batch_size,epochs,X_val, y_val) :\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(NUM_WORDS,n_dim, input_length=max_input_length))\n",
        "  model.add(SpatialDropout1D(0.2))\n",
        "  \n",
        "  model.add(Conv1D(128, 3, activation='relu'))\n",
        "  model.add(MaxPooling1D())\n",
        "  model.add(BatchNormalization())\n",
        "  \n",
        "  model.add(Conv1D(256, 3, activation='relu'))\n",
        "  model.add(MaxPooling1D())\n",
        "  model.add(BatchNormalization())\n",
        "  \n",
        "  model.add(Conv1D(512, 3, activation='relu'))\n",
        "  model.add(Conv1D(512, 3, activation='relu'))\n",
        "  model.add(Conv1D(512, 3, activation='relu'))\n",
        "  model.add(MaxPooling1D())\n",
        "  model.add(BatchNormalization())\n",
        " \n",
        "  model.add(Flatten())\n",
        "  \n",
        "  model.add(Dense(1024, activation='relu'))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  weight_file=\"cnn.alexnet.weights.best.hdf5\"\n",
        "  model_name =\"cnn.alexnet.h5\"\n",
        "  \n",
        "  callbacks_list = [EarlyStopping(monitor='val_loss', patience=2), ModelCheckpoint(weight_file, monitor='val_acc', verbose=1, save_best_only=True, mode='max')]\n",
        "  \n",
        "  model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(X_val, y_val),callbacks=callbacks_list)\n",
        "  \n",
        "  model.save(model_name)\n",
        " \n",
        "  return model_name,weight_file\n",
        "\n",
        "# model fitting and saving into drive\n",
        "\n",
        "x, y = CNN_alexnet(NUM_WORDS,n_dim,max_input_length,X_train, y_train,batch_size,epochs,X_val, y_val)\n",
        "model_evalute(x, y)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 36095 samples, validate on 4011 samples\n",
            "Epoch 1/10\n",
            "36095/36095 [==============================] - 97s 3ms/step - loss: 2.5063 - acc: 0.6012 - val_loss: 0.7135 - val_acc: 0.6487\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.64872, saving model to cnn.alexnet.weights.best.hdf5\n",
            "Epoch 2/10\n",
            "36095/36095 [==============================] - 92s 3ms/step - loss: 0.5449 - acc: 0.7693 - val_loss: 0.5026 - val_acc: 0.7806\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.64872 to 0.78060, saving model to cnn.alexnet.weights.best.hdf5\n",
            "Epoch 3/10\n",
            "36095/36095 [==============================] - 92s 3ms/step - loss: 0.3923 - acc: 0.8293 - val_loss: 0.4443 - val_acc: 0.8000\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.78060 to 0.80005, saving model to cnn.alexnet.weights.best.hdf5\n",
            "Epoch 4/10\n",
            "36095/36095 [==============================] - 92s 3ms/step - loss: 0.3455 - acc: 0.8549 - val_loss: 0.4852 - val_acc: 0.7953\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.80005\n",
            "Epoch 5/10\n",
            "36095/36095 [==============================] - 92s 3ms/step - loss: 0.2925 - acc: 0.8743 - val_loss: 0.4806 - val_acc: 0.8070\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.80005 to 0.80703, saving model to cnn.alexnet.weights.best.hdf5\n",
            "[[1803  475]\n",
            " [ 362 1816]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.79      0.81      2278\n",
            "           1       0.79      0.83      0.81      2178\n",
            "\n",
            "   micro avg       0.81      0.81      0.81      4456\n",
            "   macro avg       0.81      0.81      0.81      4456\n",
            "weighted avg       0.81      0.81      0.81      4456\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UJ33G4F9ElNB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "3b352a6a-b8e9-4ec1-daa6-63bb577ce598"
      },
      "cell_type": "code",
      "source": [
        "def CNN_vggnet(NUM_WORDS,n_dim,max_input_length,X_train, y_train,batch_size,epochs,X_val, y_val) :\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(NUM_WORDS,n_dim, input_length=max_input_length))\n",
        "  model.add(SpatialDropout1D(0.2))\n",
        "  \n",
        "  model.add(Conv1D(128, 3, activation='relu'))\n",
        "  model.add(Conv1D(128, 3, activation='relu'))\n",
        "  model.add(MaxPooling1D())\n",
        "  model.add(BatchNormalization())\n",
        "  \n",
        "  model.add(Conv1D(256, 2, activation='relu'))\n",
        "  model.add(Conv1D(256, 2, activation='relu'))\n",
        "  model.add(MaxPooling1D())\n",
        "  model.add(BatchNormalization())\n",
        "  \n",
        "  model.add(Conv1D(384, 3, activation='relu'))\n",
        "  model.add(Conv1D(384, 3, activation='relu'))\n",
        "  model.add(Conv1D(384, 3, activation='relu'))\n",
        "  model.add(MaxPooling1D())\n",
        "  model.add(BatchNormalization())\n",
        "  \n",
        "  model.add(Conv1D(512, 3, activation='relu'))\n",
        "  model.add(Conv1D(512, 3, activation='relu'))\n",
        "  model.add(Conv1D(512, 3, activation='relu'))\n",
        "  model.add(MaxPooling1D())\n",
        "  model.add(BatchNormalization())\n",
        "  \n",
        "  model.add(Conv1D(512, 3, activation='relu'))\n",
        "  model.add(Conv1D(512, 3, activation='relu'))\n",
        "  model.add(Conv1D(512, 3, activation='relu'))\n",
        "  model.add(MaxPooling1D())\n",
        "  model.add(BatchNormalization())\n",
        "  \n",
        "  model.add(Flatten())\n",
        "  \n",
        "  model.add(Dense(1024, activation='relu'))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  weight_file=\"cnn.vggnet.weights.best.hdf5\"\n",
        "  model_name =\"cnn.vggnet.h5\"\n",
        "  \n",
        "  callbacks_list = [EarlyStopping(monitor='val_loss', patience=2), ModelCheckpoint(weight_file, monitor='val_acc', verbose=1, save_best_only=True, mode='max')]\n",
        "  \n",
        "  model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(X_val, y_val),callbacks=callbacks_list)\n",
        "  \n",
        "  model.save(model_name)\n",
        "  \n",
        "  return model_name,weight_file\n",
        "\n",
        "# model fitting and saving into drive\n",
        "\n",
        "x, y = CNN_vggnet(NUM_WORDS,n_dim,max_input_length,X_train, y_train,batch_size,epochs,X_val, y_val)\n",
        "model_evalute(x, y)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 36095 samples, validate on 4011 samples\n",
            "Epoch 1/10\n",
            "36095/36095 [==============================] - 124s 3ms/step - loss: 0.7744 - acc: 0.5906 - val_loss: 0.6918 - val_acc: 0.6756\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.67564, saving model to cnn.vggnet.weights.best.hdf5\n",
            "Epoch 2/10\n",
            "36095/36095 [==============================] - 117s 3ms/step - loss: 0.5041 - acc: 0.7853 - val_loss: 0.6418 - val_acc: 0.7524\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.67564 to 0.75243, saving model to cnn.vggnet.weights.best.hdf5\n",
            "Epoch 3/10\n",
            "36095/36095 [==============================] - 117s 3ms/step - loss: 0.3706 - acc: 0.8454 - val_loss: 0.4183 - val_acc: 0.8078\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.75243 to 0.80778, saving model to cnn.vggnet.weights.best.hdf5\n",
            "Epoch 4/10\n",
            "36095/36095 [==============================] - 117s 3ms/step - loss: 0.2932 - acc: 0.8802 - val_loss: 0.4404 - val_acc: 0.8045\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.80778\n",
            "Epoch 5/10\n",
            "36095/36095 [==============================] - 117s 3ms/step - loss: 0.2712 - acc: 0.8912 - val_loss: 0.4599 - val_acc: 0.8158\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.80778 to 0.81576, saving model to cnn.vggnet.weights.best.hdf5\n",
            "[[1932  346]\n",
            " [ 449 1729]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.85      0.83      2278\n",
            "           1       0.83      0.79      0.81      2178\n",
            "\n",
            "   micro avg       0.82      0.82      0.82      4456\n",
            "   macro avg       0.82      0.82      0.82      4456\n",
            "weighted avg       0.82      0.82      0.82      4456\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6b4pg3qsJl11",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "297af7f6-fc86-43cf-c04a-134f6cdf246a"
      },
      "cell_type": "code",
      "source": [
        "def Simple_RNN(NUM_WORDS,n_dim,max_input_length,X_train,y_train,batch_size,epochs,X_val, y_val) :\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(NUM_WORDS,n_dim, input_length=max_input_length))\n",
        "  model.add(SpatialDropout1D(0.2))\n",
        "  \n",
        "  model.add(SimpleRNN(256))\n",
        "  \n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  \n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  weight_file=\"simplernn.weights.best.hdf5\"\n",
        "  model_name =\"simplernn.h5\"\n",
        "  \n",
        "  callbacks_list = [EarlyStopping(monitor='val_loss', patience=2), ModelCheckpoint(weight_file, monitor='val_acc', verbose=1, save_best_only=True, mode='max')]\n",
        "  \n",
        "  model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(X_val, y_val),callbacks=callbacks_list)\n",
        "  \n",
        "  model.save(model_name)\n",
        "  \n",
        "  return model_name,weight_file\n",
        "\n",
        "# model fitting and saving into drive\n",
        "\n",
        "x, y = Simple_RNN(NUM_WORDS,n_dim,max_input_length,X_train,y_train,batch_size,epochs,X_val, y_val)\n",
        "model_evalute(x, y)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 36095 samples, validate on 4011 samples\n",
            "Epoch 1/10\n",
            "36095/36095 [==============================] - 154s 4ms/step - loss: 0.5022 - acc: 0.7496 - val_loss: 0.3730 - val_acc: 0.8382\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.83819, saving model to simplernn.weights.best.hdf5\n",
            "Epoch 2/10\n",
            "36095/36095 [==============================] - 152s 4ms/step - loss: 0.3156 - acc: 0.8690 - val_loss: 0.4183 - val_acc: 0.8180\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.83819\n",
            "Epoch 3/10\n",
            "36095/36095 [==============================] - 151s 4ms/step - loss: 0.2758 - acc: 0.8890 - val_loss: 0.3741 - val_acc: 0.8404\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.83819 to 0.84044, saving model to simplernn.weights.best.hdf5\n",
            "[[1935  343]\n",
            " [ 340 1838]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.85      0.85      2278\n",
            "           1       0.84      0.84      0.84      2178\n",
            "\n",
            "   micro avg       0.85      0.85      0.85      4456\n",
            "   macro avg       0.85      0.85      0.85      4456\n",
            "weighted avg       0.85      0.85      0.85      4456\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "enIjqOOyVGWx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "f83b7866-fa3e-4ab0-d11e-8aee1259f5d6"
      },
      "cell_type": "code",
      "source": [
        "def RNN_LSTM(NUM_WORDS,n_dim,max_input_length,X_train,y_train,batch_size,epochs,X_val, y_val) :\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(NUM_WORDS,n_dim, input_length=max_input_length))\n",
        "  model.add(SpatialDropout1D(0.2))\n",
        "  \n",
        "  model.add(LSTM(256, dropout=0.2))\n",
        "  \n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  \n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  weight_file=\"RNN_LSTM.weights.best.hdf5\"\n",
        "  model_name =\"RNN_LSTM.h5\"\n",
        "  \n",
        "  callbacks_list = [EarlyStopping(monitor='val_loss', patience=2), ModelCheckpoint(weight_file, monitor='val_acc', verbose=1, save_best_only=True, mode='max')]\n",
        "  \n",
        "  model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(X_val, y_val),callbacks=callbacks_list)\n",
        "  \n",
        "  model.save(model_name)\n",
        "  \n",
        "  return model_name,weight_file\n",
        "\n",
        "# model fitting and saving into drive\n",
        "\n",
        "x, y = RNN_LSTM(NUM_WORDS,n_dim,max_input_length,X_train,y_train,batch_size,epochs,X_val, y_val)\n",
        "model_evalute(x, y)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 36095 samples, validate on 4011 samples\n",
            "Epoch 1/10\n",
            "36095/36095 [==============================] - 565s 16ms/step - loss: 0.4117 - acc: 0.8148 - val_loss: 0.3460 - val_acc: 0.8524\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.85241, saving model to RNN_LSTM.weights.best.hdf5\n",
            "Epoch 2/10\n",
            "36095/36095 [==============================] - 559s 15ms/step - loss: 0.2998 - acc: 0.8753 - val_loss: 0.3404 - val_acc: 0.8546\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.85241 to 0.85465, saving model to RNN_LSTM.weights.best.hdf5\n",
            "Epoch 3/10\n",
            "36095/36095 [==============================] - 558s 15ms/step - loss: 0.2582 - acc: 0.8970 - val_loss: 0.3742 - val_acc: 0.8447\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.85465\n",
            "Epoch 4/10\n",
            "36095/36095 [==============================] - 560s 16ms/step - loss: 0.2340 - acc: 0.9076 - val_loss: 0.3767 - val_acc: 0.8372\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.85465\n",
            "[[1918  360]\n",
            " [ 259 1919]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.84      0.86      2278\n",
            "           1       0.84      0.88      0.86      2178\n",
            "\n",
            "   micro avg       0.86      0.86      0.86      4456\n",
            "   macro avg       0.86      0.86      0.86      4456\n",
            "weighted avg       0.86      0.86      0.86      4456\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QtvWqHPmCdsO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "e68e08dd-cecb-43a8-caa8-2416b0c3e9dc"
      },
      "cell_type": "code",
      "source": [
        "def Stacked_LSTM(NUM_WORDS,n_dim,max_input_length,X_train,y_train,batch_size,epochs,X_val, y_val) :\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(NUM_WORDS,n_dim, input_length=max_input_length))\n",
        "  model.add(SpatialDropout1D(0.2))\n",
        "  \n",
        "  model.add(LSTM(256, dropout=0.2, return_sequences=True))\n",
        "  \n",
        "  model.add(LSTM(256, dropout=0.2))\n",
        "  \n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  \n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  weight_file=\"Stacked_LSTM.weights.best.hdf5\"\n",
        "  model_name =\"Stacked_LSTM.h5\"\n",
        "  \n",
        "  callbacks_list = [EarlyStopping(monitor='val_loss', patience=2), ModelCheckpoint(weight_file, monitor='val_acc', verbose=1, save_best_only=True, mode='max')]\n",
        "  \n",
        "  model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(X_val, y_val),callbacks=callbacks_list)\n",
        "  \n",
        "  model.save(model_name)\n",
        "  \n",
        "  return model_name,weight_file\n",
        "\n",
        "# model fitting and saving into drive\n",
        "\n",
        "x, y = Stacked_LSTM(NUM_WORDS,n_dim,max_input_length,X_train,y_train,batch_size,epochs,X_val, y_val)\n",
        "model_evalute(x, y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 36095 samples, validate on 4011 samples\n",
            "Epoch 1/10\n",
            "36095/36095 [==============================] - 1256s 35ms/step - loss: 0.3985 - acc: 0.8185 - val_loss: 0.3541 - val_acc: 0.8467\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.84667, saving model to Stacked_LSTM.weights.best.hdf5\n",
            "Epoch 2/10\n",
            "36095/36095 [==============================] - 1254s 35ms/step - loss: 0.3222 - acc: 0.8618 - val_loss: 0.3638 - val_acc: 0.8449\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.84667\n",
            "Epoch 3/10\n",
            "32000/36095 [=========================>....] - ETA: 2:18 - loss: 0.2622 - acc: 0.8949"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "16FRNU-HYoLg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def Bidirectional_LSTM(NUM_WORDS,n_dim,max_input_length,X_train,y_train,batch_size,epochs,X_val, y_val) :\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(NUM_WORDS,n_dim, input_length=max_input_length))\n",
        "  model.add(SpatialDropout1D(0.2))\n",
        "  \n",
        "  model.add(Bidirectional(LSTM(256, dropout=0.2)))\n",
        "  \n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  \n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  weight_file=\"bidirectionl.LSTM.weights.best.hdf5\"\n",
        "  model_name =\"bidirectionl.LSTM.h5\"\n",
        "  \n",
        "  callbacks_list = [EarlyStopping(monitor='val_loss', patience=2), ModelCheckpoint(weight_file, monitor='val_acc', verbose=1, save_best_only=True, mode='max')]\n",
        "  \n",
        "  model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(X_val, y_val),callbacks=callbacks_list)\n",
        "  \n",
        "  model.save(model_name)\n",
        "  \n",
        "  return model_name,weight_file\n",
        "\n",
        "# model fitting and saving into drive\n",
        "\n",
        "x, y = Bidirectional_LSTM(NUM_WORDS,n_dim,max_input_length,X_train,y_train,batch_size,epochs,X_val, y_val)\n",
        "model_evalute(x, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F8LpKegYwFfu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def Bidirectional_stacked_LSTM(NUM_WORDS,n_dim,max_input_length,X_train,y_train,batch_size,epochs,X_val, y_val) :\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(NUM_WORDS,n_dim, input_length=max_input_length))\n",
        "  model.add(SpatialDropout1D(0.2))\n",
        "  \n",
        "  model.add(Bidirectional(LSTM(256, dropout=0.2,return_sequences=True)))\n",
        "  \n",
        "  model.add(Bidirectional(LSTM(256, dropout=0.2)))\n",
        "  \n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  \n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  weight_file=\"Bidirectional_stacked_LSTM.weights.best.hdf5\"\n",
        "  model_name =\"Bidirectional_stacked_LSTM.h5\"\n",
        "  \n",
        "  callbacks_list = [EarlyStopping(monitor='val_loss', patience=2), ModelCheckpoint(weight_file, monitor='val_acc', verbose=1, save_best_only=True, mode='max')]\n",
        "  \n",
        "  model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(X_val, y_val),callbacks=callbacks_list)\n",
        "  \n",
        "  model.save(model_name)\n",
        "  \n",
        "  return model_name,weight_file\n",
        "\n",
        "# model fitting and saving into drive\n",
        "\n",
        "x, y = Bidirectional_stacked_LSTM(NUM_WORDS,n_dim,max_input_length,X_train,y_train,batch_size,epochs,X_val, y_val)\n",
        "model_evalute(x, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NNrO0fLlDzlp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, concatenate\n",
        "def multi_cnn(NUM_WORDS,n_dim,max_input_length,X_train,y_train,batch_size,epochs,X_val, y_val) :\n",
        "  \n",
        "  input_layer = Input(shape=(max_review_length,), dtype='int16', name='input') # supports integers +/- 32.7k \n",
        "  \n",
        "  embedding_layer = Embedding(NUM_WORDS,n_dim, input_length=max_input_length) (input_layer)\n",
        "  drop_embed_layer = SpatialDropout1D(0.2, name='drop_embed')(embedding_layer)\n",
        "\n",
        "  conv_1 = Conv1D(128, 3, activation='relu', name='conv_1')(drop_embed_layer)\n",
        "  maxp_1 = GlobalMaxPooling1D(name='maxp_1')(conv_1)\n",
        "\n",
        "  conv_2 = Conv1D(128, 3, activation='relu', name='conv_2')(drop_embed_layer)\n",
        "  maxp_2 = GlobalMaxPooling1D(name='maxp_2')(conv_2)\n",
        "\n",
        "  conv_3 = Conv1D(128, 3, activation='relu', name='conv_3')(drop_embed_layer)\n",
        "  maxp_3 = GlobalMaxPooling1D(name='maxp_3')(conv_3)\n",
        "\n",
        "  concat = concatenate([maxp_1, maxp_2, maxp_3])\n",
        "\n",
        "  dense_layer = Dense(256, activation='relu', name='dense')(concat)\n",
        "  drop_dense_layer = Dropout(0.2, name='drop_dense')(dense_layer)\n",
        "  dense_2 = Dense(512, activation='relu', name='dense_2')(drop_dense_layer)\n",
        "  dropout_2 = Dropout(0.2, name='drop_dense_2')(dense_2)\n",
        "\n",
        "  predictions = Dense(1, activation='sigmoid', name='output')(dropout_2)\n",
        "\n",
        "  model = Model(input_layer, predictions)\n",
        "\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  \n",
        "  weight_file=\"multi_cnn.weights.best.hdf5\"\n",
        "  model_name =\"multi_cnn.LSTM.h5\"\n",
        "  \n",
        "  callbacks_list = [EarlyStopping(monitor='val_loss', patience=2), ModelCheckpoint(weight_file, monitor='val_acc', verbose=1, save_best_only=True, mode='max')]\n",
        "  \n",
        "  model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(X_val, y_val),callbacks=callbacks_list)\n",
        "  \n",
        "  model.save(model_name)\n",
        "  \n",
        "  return model_name,weight_file\n",
        "\n",
        "# model fitting and saving into drive\n",
        "\n",
        "x, y = multi_cnn(NUM_WORDS,n_dim,max_input_length,X_train,y_train,batch_size,epochs,X_val, y_val)\n",
        "model_evalute(x, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GiA4Zb2lDzqf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fUxBFOL3Dz3Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}